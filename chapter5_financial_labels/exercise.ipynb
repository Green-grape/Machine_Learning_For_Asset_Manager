{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d2abf4",
   "metadata": {},
   "source": [
    "1. Given a time series of E-mini S&P 500 futures, compute labels on one-\n",
    "minute time bars using the fixed-horizon method, where τ is set at two\n",
    "standard deviations of one-minute returns.\n",
    "\n",
    "    1. Compute the overall distribution of the labels.\n",
    "    2. Compute the distribution of labels across all days, for each hour of the \n",
    "    trading session.\n",
    "    3. How different are the distributions in (b) relative to the distribution in (a)? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3960de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              index   Close  Volume\n",
      "0               2010-01-03 17:00:00  1113.2       1\n",
      "1               2010-01-03 17:00:00  1113.2       1\n",
      "2               2010-01-03 17:00:00  1113.2       1\n",
      "3               2010-01-03 17:00:00  1113.2       1\n",
      "4               2010-01-03 17:00:00  1113.2       1\n",
      "...                             ...     ...     ...\n",
      "2950280  2019-12-31 15:00:02.086000  3232.0       0\n",
      "2950281  2019-12-31 15:00:09.277000  3231.5       0\n",
      "2950282  2019-12-31 15:00:14.203000  3230.5       0\n",
      "2950283  2019-12-31 15:01:09.088000  3233.0       0\n",
      "2950284  2019-12-31 15:03:41.206000  3232.0       0\n",
      "\n",
      "[2950285 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "sp_db_path='../db/sp.db'\n",
    "\n",
    "con=sqlite3.connect(sp_db_path)\n",
    "\n",
    "query='''SELECT * FROM sp WHERE \"index\" BETWEEN '2010-01-01' AND '2020-12-31' '''\n",
    "\n",
    "df=pd.read_sql(query, con)\n",
    "print(df)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18aa9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index']=pd.to_datetime(df['index'], format='mixed')\n",
    "df['minute']=df['index'].dt.floor('min') # minute\n",
    "\n",
    "min_df=df.groupby('minute').agg({\n",
    "    'Close':'last',\n",
    "    'Volume':'sum'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec7e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_horizon_labels(close:pd.Series,horizon:int, tau:float)->pd.Series:\n",
    "    shifted_close=close.shift(-horizon)\n",
    "    horizon_returns=shifted_close/close - 1\n",
    "    labels=horizon_returns.apply(lambda x: (1 if x>tau else -1) if abs(x)>tau else 0)\n",
    "    return labels\n",
    "\n",
    "\n",
    "min_horizon_labels=get_fixed_horizon_labels(min_df['Close'], 20, 2*min_df['Close'].pct_change().dropna().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f463297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close\n",
      " 0    0.510622\n",
      " 1    0.253676\n",
      "-1    0.235702\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "min_label= min_horizon_labels.value_counts(normalize=True)\n",
    "print(min_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fa8df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close\n",
      " 0.0    0.474990\n",
      " 1.0    0.280764\n",
      "-1.0    0.244246\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "hour_label=min_horizon_labels.resample('h').first().value_counts(normalize=True)\n",
    "print(hour_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea24802",
   "metadata": {},
   "source": [
    "hour 단위로 뽑은것이 좀 더 signal이 많은 편이다.\n",
    "\n",
    "시장 개장/폐장, 경제지표 발표, 옵션 만기, FOMC 등의 주요 이벤트들은 대부분 정시에 맞춰서 나온다. 따라서\n",
    "1에 의해서 이벤트 직후 데이터를 잡을 확률이 높아 시그널이 좀 더 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c61bc",
   "metadata": {},
   "source": [
    "2. Repeat Exercise 1, where this time you label standardized returns \n",
    "(instead of raw returns), where the standardization is based on mean and variance\n",
    "estimates from a lookback of one hour. Do you reach a different conclusion?\n",
    "\n",
    "이전과는 반대의 결과가 나온다 (min_df)가 signal이 더 많음, normalization을 통해서 수익률의 패턴(seasonality)를 제거한 결과가 나타난것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e1d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    0.685731\n",
      "-1    0.159442\n",
      " 1    0.154826\n",
      "Name: proportion, dtype: float64\n",
      " 0.0    0.813218\n",
      "-1.0    0.094507\n",
      " 1.0    0.092275\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_fixed_normalized_horizon_labels(close:pd.Series, horizon:int, tau:int=2):\n",
    "    returns=(close.shift(-horizon)/close-1).dropna()\n",
    "\n",
    "    hour_idx=returns.index.floor('h')\n",
    "    hour_mean_returns=returns.resample('h').mean().shift(1)\n",
    "    hour_std_returns=returns.resample('h').std().shift(1)\n",
    "\n",
    "    hour_mean_aligned=hour_idx.map(hour_mean_returns)\n",
    "    hour_std_aligned=hour_idx.map(hour_std_returns)\n",
    "\n",
    "    # normalize returns\n",
    "    returns_normalized = (returns - hour_mean_aligned) / hour_std_aligned\n",
    "\n",
    "    labels=returns_normalized.apply(lambda x: (1 if x>tau else -1) if abs(x)>tau else 0)\n",
    "\n",
    "    return labels\n",
    "\n",
    "normalized_min_horizon_labels=get_fixed_normalized_horizon_labels(min_df['Close'], 20, 2)\n",
    "\n",
    "normalized_min_label=normalized_min_horizon_labels.value_counts(normalize=True)\n",
    "normalized_hour_label=normalized_min_horizon_labels.resample('h').first().value_counts(normalize=True)\n",
    "print(normalized_min_label)\n",
    "print(normalized_hour_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae477be",
   "metadata": {},
   "source": [
    "3. Repeat Exercise 1, where this time you apply the triple-barrier method on\n",
    "volume bars. The maximum holding period is the average number of bars per\n",
    "day, and the horizontal barriers are set at two standard deviations of bar\n",
    "returns. How do results compare to the solutions from Exercises 1 and 2?\n",
    "\n",
    "거의 2개의 결과가 비슷한것으로 보아서 seasonality의 해결뿐만 아니라 실제로 labeling 중간에 발생하는 신호(중간 이벤트)를 무시하지 않도록 해주기 때문이다. 또한 0보다 1, -1 label이 많아서 보다 시장의 기회를 잘 포착하는 것을 확인할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58dd954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minute\n",
      "2010-01-03 17:00:00   2010-01-03 21:39:00\n",
      "2010-01-03 17:01:00   2010-01-03 21:39:00\n",
      "2010-01-03 17:02:00   2010-01-03 21:39:00\n",
      "2010-01-03 17:03:00   2010-01-03 22:04:00\n",
      "2010-01-03 17:04:00   2010-01-03 22:04:00\n",
      "                              ...        \n",
      "2019-12-30 15:03:00   2019-12-31 02:21:00\n",
      "2019-12-30 15:07:00   2019-12-31 02:21:00\n",
      "2019-12-31 02:21:00   2019-12-31 08:30:00\n",
      "2019-12-31 08:30:00   2019-12-31 14:48:00\n",
      "2019-12-31 08:54:00   2019-12-31 14:48:00\n",
      "Name: minute, Length: 856970, dtype: datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/857 [07:11<?, ?it/s]\n",
      "  0%|          | 0/857 [01:02<?, ?it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 857/857 [01:02<00:00, 13.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "import statsmodels.api as sm\n",
    "import multiprocessing\n",
    "\n",
    "def split_index(index: pd.Index, chunk_size: int = 1000):\n",
    "    for i in range(0, len(index), chunk_size):\n",
    "        yield index[i:i + chunk_size]\n",
    "\n",
    "min_df_daily_bar=min_df.groupby(min_df.index.date).count()\n",
    "average_daily_bars=int(round(min_df_daily_bar.mean().iloc[0]))\n",
    "\n",
    "# set vertical barrier\n",
    "t1=min_df.index.searchsorted(min_df.index+pd.Timedelta(minutes=average_daily_bars))\n",
    "t1=t1[t1<len(min_df)]\n",
    "t1=pd.Series(data=min_df.index[t1], index=min_df.index[:len(t1)])\n",
    "print(t1)\n",
    "\n",
    "\n",
    "def get_triple_barrier_label(close: pd.Series, t1: pd.Series, barrier_width: Tuple[float, float], molecule: pd.Index = None):\n",
    "    if molecule is not None:\n",
    "        close = close[molecule].copy()\n",
    "        t1 = t1[molecule].copy()\n",
    "\n",
    "    upper_barrier_width, lower_barrier_width = barrier_width\n",
    "\n",
    "    # datetime64[ns]로 미리 선언 (NaT로 채워짐)\n",
    "    ret = pd.DataFrame(index=t1.index)\n",
    "    ret['t1'] = pd.to_datetime(t1)\n",
    "    ret['sl'] = pd.NaT\n",
    "    ret['pt'] = pd.NaT\n",
    "\n",
    "    for h_s, h_e in t1.fillna(close.index[-1]).items():\n",
    "        path_price = close.loc[h_s:h_e]\n",
    "        if len(path_price) == 0:\n",
    "            continue\n",
    "\n",
    "        path_return = (path_price / path_price.iloc[0] - 1).dropna()\n",
    "\n",
    "        # 하단/상단 배리어 최초 터치 시점 (없으면 NaT 유지)\n",
    "        sl_idx = path_return.index[path_return <= -lower_barrier_width]\n",
    "        pt_idx = path_return.index[path_return >=  upper_barrier_width]\n",
    "        ret.loc[h_s, 'sl'] = sl_idx.min() if len(sl_idx) else pd.NaT\n",
    "        ret.loc[h_s, 'pt'] = pt_idx.min() if len(pt_idx) else pd.NaT\n",
    "\n",
    "    # 세 컬럼 모두 datetime64[ns] → 안전하게 min(axis=1)\n",
    "    first_touch = ret[['t1', 'sl', 'pt']].min(axis=1)\n",
    "\n",
    "    # 레이블 부여\n",
    "    ret['label'] = np.select(\n",
    "        [first_touch.eq(ret['t1']), first_touch.eq(ret['sl']), first_touch.eq(ret['pt'])],\n",
    "        [0, -1, 1],\n",
    "        default=0\n",
    "    )\n",
    "    ret['t1'] = first_touch\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_triple_barrier_label_molecule(args):\n",
    "    close, t1, (sl, pt), molecule=args\n",
    "    return get_triple_barrier_label(close, t1, (sl, pt), molecule)\n",
    "\n",
    "n_jobs = multiprocessing.cpu_count() - 1\n",
    "molecules = list(split_index(t1.index, chunk_size=1000))\n",
    "tau=2*min_df['Close'].pct_change().dropna().std()\n",
    "with tqdm_joblib(tqdm(total=len(molecules))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_triple_barrier_label_molecule)((min_df['Close'].loc[t1.index], t1, (tau, tau), mol))\n",
    "        for mol in molecules\n",
    "    )\n",
    "\n",
    "triple_barrier_label=pd.concat(results).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad862bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      " 1    0.478149\n",
      "-1    0.458651\n",
      " 0    0.063201\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      " 1.0    0.445743\n",
      "-1.0    0.418892\n",
      " 0.0    0.135365\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "min_triple_barrier_labels=triple_barrier_label['label'].value_counts(normalize=True)\n",
    "hour_triple_barrier_labels=triple_barrier_label.resample('h').first()['label'].value_counts(normalize=True)\n",
    "\n",
    "print(min_triple_barrier_labels)\n",
    "print(hour_triple_barrier_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a46e7",
   "metadata": {},
   "source": [
    "4. Repeat Exercise 1, where this time you apply the trend-scanning method,\n",
    "with look-forward periods of up to one day. How do results compare to the\n",
    "solutions from Exercises 1, 2, and 3?\n",
    "\n",
    "t-value의 부호를 기준으로 label을 부여하므로 0이라는 label이 존재하지 않는 다는 것이 가장 큰 차이이고 3과 비슷하게 seasonality에 덜 민감합니다. (고정된 시간 대신, 추세를 직접 측정하기 때문에)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23cebf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/857 [02:53<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [06:09<00:00, 36.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "import statsmodels.api as sm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_t_val_linear(close:pd.Series):\n",
    "    x=np.ones((len(close), 2))\n",
    "    x[:, 1]=np.arange(len(close))\n",
    "    ols=sm.OLS(close, x).fit()\n",
    "    return ols.tvalues[1]\n",
    "\n",
    "def _tval_for_span(cur_index, cur_end_iloc, close):\n",
    "    span_close_end = close.index[cur_end_iloc]\n",
    "    span_close = close.loc[cur_index:span_close_end]\n",
    "    tval = get_t_val_linear(span_close.values)\n",
    "    return span_close_end, tval\n",
    "\n",
    "def get_bins_from_trend(module:pd.Index, close:pd.Series, max_look_forward:pd.Timedelta, n_threads:int=8):\n",
    "    '''\n",
    "    linear trend로 부터 t-value의 sign을 구한다. \n",
    "    '''\n",
    "    ret=pd.DataFrame(index=module, columns=['t1', 't_val', 'bin'])\n",
    "    for cur_index in module:\n",
    "        t_vals=pd.Series()\n",
    "        start_iloc=close.index.get_loc(cur_index)\n",
    "        end_iloc=np.searchsorted(close.index, cur_index+max_look_forward, side='right')\n",
    "        end_range = range(start_iloc + 5, min(len(close.index), end_iloc))\n",
    "        if start_iloc+5>=min(len(close.index), end_iloc):\n",
    "            continue\n",
    "\n",
    "        # 스레드로 내부 t-value 계산\n",
    "        with ThreadPoolExecutor(max_workers=n_threads) as ex:\n",
    "            results = list(ex.map(lambda i: _tval_for_span(cur_index, i, close), end_range))\n",
    "\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        idx, vals = zip(*results)\n",
    "        t_vals = pd.Series(vals, index=pd.Index(idx, name='span_end'), dtype='float64')\n",
    "        t_vals = pd.to_numeric(t_vals, errors='coerce')\n",
    "        finite = t_vals[np.isfinite(t_vals.to_numpy())]\n",
    "        if finite.empty:\n",
    "            continue\n",
    "        max_abs_t_val_idx = finite.abs().idxmax()\n",
    "        ret.loc[cur_index, ['t1', 't_val', 'bin']]=finite.index[-1], finite[max_abs_t_val_idx], np.sign(finite[max_abs_t_val_idx])\n",
    "    ret['t1']=pd.to_datetime(ret['t1'])\n",
    "    ret['bin']=pd.to_numeric(ret['bin'], downcast='signed')\n",
    "    return ret.dropna(subset=['bin'])\n",
    "\n",
    "def get_bins_from_trend_molecule(args):\n",
    "    molecule, close, max_look_forward = args\n",
    "    return get_bins_from_trend(molecule, close, max_look_forward)\n",
    "\n",
    "import multiprocessing\n",
    "    \n",
    "n_jobs = multiprocessing.cpu_count() - 1\n",
    "max_look_forward = pd.Timedelta(days=1)\n",
    "\n",
    "molecules = list(split_index(min_df.index[:10000], chunk_size=1000))\n",
    "with tqdm_joblib(tqdm(total=len(molecules))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_bins_from_trend_molecule)((mol, min_df['Close'], max_look_forward))\n",
    "        for mol in molecules\n",
    "    )\n",
    "\n",
    "# 결과 합치기\n",
    "trend_label_result = pd.concat(results).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d5c39b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\n",
      "-1.0    0.517217\n",
      " 1.0    0.482783\n",
      "Name: proportion, dtype: float64\n",
      "bin\n",
      "-1.0    0.516224\n",
      " 1.0    0.483776\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(trend_label_result['bin'].value_counts(normalize=True))\n",
    "print(trend_label_result.resample('h').first()['bin'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e90c2",
   "metadata": {},
   "source": [
    "5. Using the labels generated in Exercise 3 (triple-barrier method):\n",
    "    1. a Fit a random forest classifier on those labels. Use as features estimates of\n",
    "    mean return, volatility, skewness, kurtosis, and various differences in moving averages.\n",
    "    2. Backtest those predictions using as a trading rule the same rule used to\n",
    "    generate the labels.\n",
    "    3. Apply meta-labeling on the backtest results.\n",
    "    4. Refit the random forest on meta-labels, adding as a feature the label predicted in (a).\n",
    "    5. Size (a) bets according to predictions in (d), and recompute the backtest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adaa20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AFML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
